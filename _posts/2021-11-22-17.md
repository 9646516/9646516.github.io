---
layout: post
title: "transformer"
categories:
  - Category
---

这几天实在卷不动了，去看了看transformer相关。看了几篇重要的文章之后，我的感受只有两个字：就这？

让我们从卷积开始，只有理解了卷积在干什么，我们才知道transformer的思路。首先不管卷积层是不是在做卷积，卷积神经网络是从传统手工方法出发的。过去的方法往往十分重视边缘，而边缘的特征就是梯度，所以最经典的方法是求Jacobian和Hessian矩阵。这种方法过于重视局部，求出来的往往是一个个关键点和这些点周围的梯度信息。如果要解决下游任务，需要将合理使用这些关键点，如关键点之间匹配或者打包之后分类。卷积神经网络是这种思路的延续，之前的手工方法由于是在局部求导数，导致看不到全局的信息。卷积神经网络通过堆叠卷积层，并且引入池化的方法，当层数到达一个级别后就可以看到关键目标的全貌。

vision transformer本质还是离不开卷积，虽然说现在有全transformer的工作，但是这些工作第一步都是切patch。而切patch+求QKV的过程等价于一个stride等于patch size 的卷积。但是随着transformer块的堆叠，到后面会和卷积完全不一样。总有人说transformer会丢失位置信息，我觉得说法并不准确，真实情况应该是随着transformer的深入，位置信息会被遗忘。当下做transformer似乎都在向卷积神经网络靠拢。做法包括局部注意力、把transformer N\*d的特征reshape成H \* W \*d，然后用类似卷积的操作。转了一圈最后回到了卷积，ResNet领先业界十年实锤了。

同时位置编码是个很恶心的东西，这个是transformer网络本身的问题，他没有显式给出位置信息，网络自己也学不会，只能强行加上PE。但是视觉任务和文字处理不一样，应该有显式给出位置信息的方法。

个人感觉现在transformer的工作并不自然，因为他不是像卷积那样一步一步发展而来的，而是强行被从隔壁搬来的。虽然他效果不错，但谷歌年初弄的纯MLP网络做视觉效果也不错，不能用效果证明一个方法合理。而transformer效果不错的原因，本质还是网络复杂度高，这里说的不是参数多或者计算复杂度高，而是说网络没有引入一些先验知识。众所周知dropout可以解决过拟合问题，但是dropout却多用于全连接层而不是卷积层，原因就是多层的全连接层复杂度高。高复杂度会提升网络的容量，但是会导致难以训练的问题，这也是当前transformer需要海量数据的原因。
