---
layout: post
title: "最近的各种无监督学习"
categories:
  - Category
---

这两年无监督学习发展的真的很快，对比学习从开始远不如监督学习一路发展到了接近监督学习的水平。而最近很多人开始弄的mask autoencoder,效果也是好的不可思议。

我是很喜欢对比学习的，对比学习遵从的autoencoder的思路没有人工的痕迹，而且事实证明即使不对形式加以先验的约束，也能达到极好的效果。而且对比学习学出来的特征更适合去表示对象本质的特征。

比如前阵子很火的声音克隆的项目，那个项目实现的论文其实是2019年的老论文了，它就是用了类似对比学习的方法实现了一个voice encoder来学习人声音的特征，这种特征比人工设计的MFCC之类的频谱特征更加适合嵌入到tts的pipeline中，因此论文实际上直接使用cat来嵌入人的声音特征。

又或者是corner net中用半监督得到的特征来判断两个角是不是一对。其实对比学习早就广泛使用了，只是没有明确提出来。

当前用对比学习学出来特征表示，然后拆掉最后的fc层用前面的feature来做下游任务，其实是有些浪费了，当前在做一些进一步的工作，这里不多展开。相比之下，MAE训练过程中的脚手架，则是用处很小，训练出来的encoder似乎也只能用来做下游任务。

但是对比学习的最大问题，是损失的设计。损失是一个pipeline中最重要的部分，它约束了网络学习的方向。个人觉得显示给出负样本的对比学习，其实比不包含负样本的对比学习使用起来更加实际一点。当然，如果数据足够多且计算资源不要钱的话，选后者肯定没问题。
